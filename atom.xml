<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Skye Huang&#39;s Blog</title>
  
  
  <link href="http://example.com/atom.xml" rel="self"/>
  
  <link href="http://example.com/"/>
  <updated>2020-10-10T08:17:56.598Z</updated>
  <id>http://example.com/</id>
  
  <author>
    <name>Skye Huang</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Project Summary</title>
    <link href="http://example.com/2020/10/10/Project-Summary/"/>
    <id>http://example.com/2020/10/10/Project-Summary/</id>
    <published>2020-10-10T08:04:50.000Z</published>
    <updated>2020-10-10T08:17:56.598Z</updated>
    
    <content type="html"><![CDATA[<p><strong>Skye Huang</strong></p><p><strong>5 March 2020</strong></p><p class="p">Breast cancer has skyrocketed to the top of the list of most common cancer, right after for skin cancer. Around one in eight women will develop breast cancer in her lifetime and so does every one man in a thousand. Even though the number of people suffering from breast cancer is increasing every day, scientists still haven't figured out the cause of this disease. However, some measures have been developed to predict and prevent breast cancer before it is too late. Thus, using a data set that consists of some additional information about breast cancer, I will train a binary classification model to predict whether the cancer is benign or malignant through machine learning. Machine learning is a phrase which we refer to as the "learning styles" of a computer. It could either learn in a supervised way or an unsupervised way. In terms of the problem that I am addressing, a supervised machine learning would best fit because we have a labeled dataset. I used the dataset that contains thirty features obtained from a digitized image of a fine needle aspirate (FNA) of a breast mass, where we have 357 benign samples and 212 malignant samples.</p><p class="p">Following the general Machine Learning Pipeline, approximately 10 data fields were chosen from the dataset, including radius, texture, perimeter, area, and so on.</p><p class="p">The package Scikit-learn (later referred to as sk-learn) allowed me to introduce some aspect of the machine learning mechanism to my code. But, because the data set consists of too large an amount of data to comprehend in detail’ upon receiving the data, I interpreted the data through the means of data exploration, which created a big picture of trends and major points of the data. Using tools like Matplotlib and Seaborn in Python, there were graphs to visualize the data. These graphs help me see the relationships between data and thus give me ideas about how to perform my data cleaning.</p><p class="p">Feature engineering is the process of using domain knowledge of the data to create or modify features to make machine learning algorithms work. Thus,  standardscaler, which standardizes features by removing the mean and scaling to unit variance, became essential for comparing and choosing between different features that would be most helpful for machine training. This is a process called data normalization, and it adjusts values measured on different scales to notionally common scales, makes the data comparable, and makes the model training results better. During the feature engineering process, it is also important to encode categorical features to make the data understandable to ML algorithms.</p><p class="p">Graphs like violin plots and heat maps would provide a great graphical description of the data collected. More specifically, the split violins in the violin plot compare the distributions of each group, showing the impact of each data. On the other hand, correlation coefficients are used to measure the strength of the relationship between two variables. We can compute the correlation coefficients and plot them in a heatmap then discard high relevant features.</p><p class="p">While diving into feature selection through graphs and tables created through the algorithms, the importance of influences of different features have shown their importance. In this case, from the plot “radius_mean distribution of benign and malignant”, it’s clear that benign and malignant have different radius_mean distribution. Therefore radius_mean can be considered to be a useful predictor for whether breast cancer is benign or malignant, from which, after examining other features of the data, area and perimeter also presented themselves to be a useful feature when differentiating the type of breast cancer. </p><p class="p">After choosing the appropriate features to work with, the idea of recursive feature elimination was introduced to the study. Recursive feature elimination is based on the idea to repeatedly construct a model (for example an SVM or a regression model) and choose either the best or worst performing feature (for example based on coefficients), setting the feature aside and then repeating the process with the rest of the features. This process is applied until all features in the dataset are exhausted. Features are then ranked according to when they were eliminated. As such, it is a greedy optimization for finding the best performing subset of features.</p><p class="p">Following, the next step was to choose a model -- A machine learning model is usually a mathematical or statistical algorithm with trained parameters over a training class. While most of the models are complex, they are usually a combination of simple models or modified simple models with special techniques. Some examples of models would be linear regression, logistic regression, k-nearest neighbors algorithm (kNN), and the decision tree. At this point, we need to consider which machine learning model would perform the best.</p><p class="p">Linear regression is a linear approach to modeling the relationship between a dependent variable and one or more independent variables. However, the label of a binary classification, which is our problem, is 0 or 1, and the linear regression can’t solve a binary classification problem and is ruled out.</p><p><img src="/2020/10/10/Project-Summary/1.png" alt="1"></p><p class="p">Logistic regression trains preprocessed data and eventually predicts its outcome. It classifies data into two and only two outputs and is used in binary classification problems like this one very often. Thus, after visualizing the prediction yielded from the logistic regression model, we tested the performance of the model, including its accuracy, precision, recall, and F-1 score of different models.</p><p class="p">The third model is kNN, k-nearest neighbors algorithm (kNN), which is a method used for classification. The input consists of the k closest training examples in the feature space, the output is a class. An object is classified by a plurality vote of its neighbors, with the object being assigned to the class most common among its k nearest neighbors (k is a positive integer, typically small). If k = 1, then the object is simply assigned to the class of that single nearest neighbor. After running the code, the performance of the model was also recorded to be compared between the different models for the best model.</p><p class="p">The last model we tested is the decision tree, which is a flowchart-like structure in which each internal node represents a “test” on an attribute (e.g. whether a coin flip comes up heads or tails), each branch represents the outcome of the test, and each leaf node represents a class label (decision taken after computing all attributes). The paths from the root to the leaf represent classification rules. According to the attributes of the data, decision models were made with tree structures with the additional parameter of “max_depth” to avoid overflowing and its performance was recorded.</p><p class="p">After testing out all the options of different machine models, we need to compare and interpret the results. Cross-validation was fitting for this situation as it is a resampling procedure used to evaluate machine learning models on a limited data sample. It can also prevent overfitting and is an accountable way to evaluate the performance of a model with training data only. The next step is optimization: tuning different parameters to get better prediction results; for example, the parameter p in KNN, the parameter n_neighbors in KNN, the parameter c in logistic regression. At last, the decision tree yielded the best scores of all with an accuracy score of  0.936, a precision score of 0.894, a recall score of 0.937, and an F1-score of 0.915. </p><p class="p">All in all, this project used a binary classification model to predict benign and malignant breast cancers based on different features through the decision tree model. </p> <style>    .p {        text-indent:2em;        text-align:justify;    } </style>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;&lt;strong&gt;Skye Huang&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;5 March 2020&lt;/strong&gt;&lt;/p&gt;
&lt;p class=&quot;p&quot;&gt;
Breast cancer has skyrocketed to the top of the list o</summary>
      
    
    
    
    
  </entry>
  
</feed>
